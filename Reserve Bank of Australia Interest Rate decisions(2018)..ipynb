{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The exercise below uses two off-the-shelf NLP models developed independently by Google and Facebook. Both models are trained to learn to read interest rate decisions by the Reserve Bank of Australia. Both models are then tested to see how they do on simple word association tasks i.e  has the algorithm learnt to 'read' the vocabulary of the Reserve Bank and has it grouped vocabulary into the correct contexts. \n",
    "\n",
    "#### Beyond the scope of this exercise is the use of the resulting  word embedding matrix for downstream Regression or Classification tasks.  As a side note TFIDF models are an alternate route for basic sentiment analysis tasks especially where sentiment is explicit in the document. Personal preference is to use variants of RNN's  in tailored NLP tasks and where possible keep the net as shallow as possible depending on the data and task.\n",
    "\n",
    "\n",
    "#### Aug/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.en import English\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import FastText \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.manifold\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import math\n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use SpaCy to parse data. Leaving stopwords in, just Lemmatizing. Not Stemming.\n",
    "  \n",
    "\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "parser = English() \n",
    "\n",
    "\n",
    "corpus_list_train = []\n",
    "import codecs\n",
    "with codecs.open('..RBA_2018.txt','r',encoding='utf8') as f:\n",
    "     for line in f.readlines():\n",
    " \n",
    "        article= []\n",
    "         \n",
    "        corpus_test  = nlp(line) \n",
    "        for w in corpus_test:\n",
    "            if not w.is_punct:\n",
    "                 article.append(w.lemma_)\n",
    " \n",
    "        corpus_list_train.append(article)\n",
    " \n",
    " \n",
    "X_train = corpus_list_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269614"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word2vec. Model designed by the Google Brain Team(Mikolov et al) and released around 2013/14. \n",
    "#Models developed in Python by the team at Gensim.\n",
    " \n",
    "size = 200\n",
    "window_size = 5  \n",
    "epochs = 50\n",
    "min_count = 0\n",
    "workers = 4\n",
    "\n",
    " \n",
    "model = word2vec.Word2Vec(X_train, alpha=0.015,sg=1,window=window_size,size=size,\\\n",
    "                          min_count=min_count,workers=workers,iter=epochs,\n",
    "                          sample=0.01,batch_words=1,negative=5)\n",
    "\n",
    "model.build_vocab(sentences=X_train,update=True)\n",
    "model.train(sentences=X_train,epochs=50,total_examples=model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fasttext model. Mikolov et al. Mikolov moved to the Facebook AI team and model released\n",
    "#a couple of years ago. Model moves beyond Word2Vec and breaks words into sub component \n",
    "#morphological parts which also strives to makes it easier to use on multiple languages.\n",
    "\n",
    "model_ft = FastText(X_train,sg=0, hs=0, size=200, alpha=0.025, \n",
    "                     window=5, min_count=0, max_vocab_size=None, word_ngrams=1,\n",
    "                     sample=0.001, seed=1, workers=4, min_alpha=0.0001, negative=5, \n",
    "                     cbow_mean=1, iter=5, null_word=0,\n",
    "                     min_n=2, max_n=8, sorted_vocab=1, bucket=2000000, trim_rule=None, batch_words=300) \n",
    "\n",
    "model_ft.build_vocab(sentences=X_train,update=True)\n",
    "model_ft.train(sentences=X_train,epochs=50,total_examples=model_ft.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'cash', 0.9738770723342896),\n",
       " (u'decide', 0.9602638483047485),\n",
       " (u'today', 0.8722482919692993),\n",
       " (u'board', 0.8350348472595215),\n",
       " (u'1.50', 0.7566527724266052),\n",
       " (u'unchanged', 0.7442362308502197),\n",
       " (u'stance', 0.7228062152862549),\n",
       " (u'meeting', 0.7142150402069092),\n",
       " (u'hold', 0.7089430689811707),\n",
       " (u'which', 0.6934565305709839)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word association using Word2Vec.\n",
    "#Checking what the model believes are the ten most contextually\n",
    "#similar words to the word 'leave'.\n",
    " \n",
    "model.wv.most_similar(('leave'),topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'cash', 0.9998171925544739),\n",
       " (u'unchanged', 0.999815046787262),\n",
       " (u'board', 0.9998142123222351),\n",
       " (u'cent', 0.9998129606246948),\n",
       " (u'capacity', 0.999812126159668),\n",
       " (u'boost', 0.9998111128807068),\n",
       " (u'leave', 0.999811053276062),\n",
       " (u'bank', 0.9998108744621277),\n",
       " (u'number', 0.9998099207878113),\n",
       " (u'policy', 0.9998087882995605)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word association using FastText.\n",
    "#What does the model believe are the ten most contextually similar words/terms to '1.5'\n",
    "#Current Cash rate set by the Reserve Bank is 1.5%.\n",
    "\n",
    "\n",
    "model_ft.wv.most_similar(('1.50'),topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 <gensim.models.keyedvectors.EuclideanKeyedVectors object at 0x118871250>\n",
      "200 <gensim.models.wrappers.fasttext.FastTextKeyedVectors object at 0x12481a190>\n"
     ]
    }
   ],
   "source": [
    "#Word embedding matrix and unique Vocabulary index table from word2vec and fasttext models\n",
    "#for downstream regression or classification algorithms.\n",
    "\n",
    "print(model.vector_size, model.wv )\n",
    "print(model_ft.vector_size, model_ft.wv )\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))\n",
    "ft = dict(zip(model_ft.wv.index2word, model_ft.wv.syn0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
